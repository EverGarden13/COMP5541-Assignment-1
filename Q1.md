# Question 1: Forward and Backward Process of Neural Network

## Given Information

**Network Architecture:**
- Input layer: 3 neurons
- Hidden layer: 4 neurons with weights W₁
- Output layer: 3 neurons with weights W₂
- Hidden layer activation: ReLU(x) = max(0,x)
- Output layer activation: softmax
- Loss function: Cross-entropy L_CE

**Given Values:**
- Input: x = [0.5, -1.0, 2]ᵀ
- Label: y = [1, 1, 0]ᵀ

**Weight Matrices:**
```
W₁ = [0.2  -0.3   0.4]
     [0.1   0.2  -0.5]
     [-0.3  0.1   0.2]
     [0.4  -0.1   0.3]

W₂ = [0.1   0.2  -0.1   0.1]
     [-0.2  0.3   0.1  -0.3]
     [0.2  -0.1   0.3   0.2]
```

## Part (a): Forward Process

### Step 1: Calculate Hidden Layer Input (z₁)
z₁ = W₁x

```
z₁ = [0.2  -0.3   0.4] [0.5]   [0.2×0.5 + (-0.3)×(-1.0) + 0.4×2]   [1.20]
     [0.1   0.2  -0.5] [-1.0] = [0.1×0.5 + 0.2×(-1.0) + (-0.5)×2] = [-1.15]
     [-0.3  0.1   0.2] [2]     [(-0.3)×0.5 + 0.1×(-1.0) + 0.2×2]   [0.150]
     [0.4  -0.1   0.3]         [0.4×0.5 + (-0.1)×(-1.0) + 0.3×2]   [0.900]
```

### Step 2: Apply ReLU Activation to Get Hidden Representation (h)
h = ReLU(z₁) = max(0, z₁)

```
h = [max(0, 1.20)]   [1.20]
    [max(0, -1.15)] = [0]
    [max(0, 0.150)]   [0.150]
    [max(0, 0.900)]   [0.900]
```

### Step 3: Calculate Output Layer Input (z₂)
z₂ = W₂h

```
z₂ = [0.1   0.2  -0.1   0.1] [1.20]   [0.1×1.20 + 0.2×0 + (-0.1)×0.150 + 0.1×0.900]   [0.195]
     [-0.2  0.3   0.1  -0.3] [0]    = [(-0.2)×1.20 + 0.3×0 + 0.1×0.150 + (-0.3)×0.900] = [-0.495]
     [0.2  -0.1   0.3   0.2] [0.150]  [0.2×1.20 + (-0.1)×0 + 0.3×0.150 + 0.2×0.900]     [0.465]
                              [0.900]
```

### Step 4: Apply Softmax to Get Output Prediction (ŷ)
ŷ = softmax(z₂)

First, calculate exponentials:
```
e^z₂ = [e^0.195]   [1.215]
       [e^(-0.495)] = [0.610]
       [e^0.465]    [1.592]
```

Sum of exponentials: Σe^z₂ = 1.215 + 0.610 + 1.592 = 3.417

```
ŷ = [1.215/3.417]   [0.356]
    [0.610/3.417] = [0.179]
    [1.592/3.417]   [0.466]
```

**Final Results for Part (a):**
- Hidden representation: h = [1.20, 0, 0.150, 0.900]ᵀ
- Output prediction: ŷ = [0.356, 0.179, 0.466]ᵀ

## Part (b): Backward Propagation

### Step 1: Calculate Cross-Entropy Loss
L_CE = -Σᵢ yᵢ ln(ŷᵢ) = -[1×ln(0.356) + 1×ln(0.179) + 0×ln(0.466)]
L_CE = -[-1.032 + (-1.721) + 0] = 2.753

### Step 2: Calculate ∂L_CE/∂z₂ (Output Layer Gradient)
For cross-entropy loss with softmax: ∂L_CE/∂z₂ = ŷ - y

```
∂L_CE/∂z₂ = [0.356]   [1]   [-0.644]
            [0.179] - [1] = [-0.821]
            [0.466]   [0]   [0.466]
```

### Step 3: Calculate ∂L_CE/∂W₂
∂L_CE/∂W₂ = (∂L_CE/∂z₂) × hᵀ

```
∂L_CE/∂W₂ = [-0.644]                    [-0.644×1.20  -0.644×0  -0.644×0.150  -0.644×0.900]
            [-0.821] × [1.20  0  0.150  0.900] = [-0.821×1.20  -0.821×0  -0.821×0.150  -0.821×0.900]
            [0.466]                     [0.466×1.20   0.466×0   0.466×0.150   0.466×0.900]

∂L_CE/∂W₂ = [-0.773   0      -0.0966  -0.580]
            [-0.985   0      -0.123   -0.739]
            [0.559    0       0.0699   0.419]
```

### Step 4: Calculate ∂L_CE/∂h (Hidden Layer Gradient)
∂L_CE/∂h = W₂ᵀ × (∂L_CE/∂z₂)

```
∂L_CE/∂h = [0.1  -0.2   0.2] [-0.644]   [0.1×(-0.644) + (-0.2)×(-0.821) + 0.2×0.466]   [0.193]
           [0.2   0.3  -0.1] [-0.821] = [0.2×(-0.644) + 0.3×(-0.821) + (-0.1)×0.466] = [-0.422]
           [-0.1  0.1   0.3] [0.466]   [(-0.1)×(-0.644) + 0.1×(-0.821) + 0.3×0.466]   [0.122]
           [0.1  -0.3   0.2]           [0.1×(-0.644) + (-0.3)×(-0.821) + 0.2×0.466]   [0.275]
```

### Step 5: Apply ReLU Derivative
∂L_CE/∂z₁ = ∂L_CE/∂h ⊙ ReLU'(z₁)

ReLU'(z₁) = [1, 0, 1, 1]ᵀ (since z₁ = [1.20, -1.15, 0.150, 0.900]ᵀ)

```
∂L_CE/∂z₁ = [0.193]   [1]   [0.193]
            [-0.422] ⊙ [0] = [0]
            [0.122]    [1]   [0.122]
            [0.275]    [1]   [0.275]
```

### Step 6: Calculate ∂L_CE/∂W₁
∂L_CE/∂W₁ = (∂L_CE/∂z₁) × xᵀ

```
∂L_CE/∂W₁ = [0.193]                    [0.193×0.5  0.193×(-1.0)  0.193×2]
            [0]      × [0.5  -1.0  2] = [0×0.5      0×(-1.0)      0×2]
            [0.122]                     [0.122×0.5  0.122×(-1.0)  0.122×2]
            [0.275]                     [0.275×0.5  0.275×(-1.0)  0.275×2]

∂L_CE/∂W₁ = [0.0965  -0.193   0.386]
            [0        0        0]
            [0.0610  -0.122    0.244]
            [0.138   -0.275    0.550]
```

## Final Answers

**Part (a):**
- Hidden representation: h = [1.20, 0, 0.150, 0.900]ᵀ
- Output prediction: ŷ = [0.356, 0.179, 0.466]ᵀ

**Part (b):**
- ∂L_CE/∂W₂ = [-0.773   0      -0.0966  -0.580]
              [-0.985   0      -0.123   -0.739]
              [0.559    0       0.0699   0.419]

- ∂L_CE/∂W₁ = [0.0965  -0.193   0.386]
              [0        0        0]
              [0.0610  -0.122    0.244]
              [0.138   -0.275    0.550] 