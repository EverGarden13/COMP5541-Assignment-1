{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# COMP5541 Assignment 1 - Question 3: Transfer Learning\n",
    "\n",
    "In this notebook, we will explore transfer learning to improve model classification performance on the CIFAR-10 dataset using ImageNet pre-trained models from PyTorch Model Zoo.\n",
    "\n",
    "## Tasks:\n",
    "- Fine-tune ImageNet pre-trained AlexNet with different amounts of training data (10%, 20%, 50%)\n",
    "- Compare with training from scratch\n",
    "- Fine-tune other pre-trained models (ResNet18 and VGG16)\n",
    "- Explore fine-tuning only portions of network layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from torch.utils.data import Subset\n",
    "from torchvision.models import alexnet, AlexNet_Weights\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup Environment\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Preparation and Loading\n",
    "\n",
    "# Data Preparation\n",
    "def load_cifar10(batch_size=128):\n",
    "    # Define transforms for training data\n",
    "    # Note: CIFAR-10 images are 32x32 but models like AlexNet expect 224x224\n",
    "    # We'll resize the images to match the expected input size\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.RandomCrop(224, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    # Load CIFAR-10 dataset\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=True, download=True, transform=transform_train)\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=False, download=True, transform=transform_test)\n",
    "\n",
    "    # Create data loaders\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    # CIFAR-10 classes\n",
    "    classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "    \n",
    "    return trainset, testset, trainloader, testloader, classes\n",
    "\n",
    "# Create subset of training data with specific percentage\n",
    "def create_subset(dataset, percentage):\n",
    "    \"\"\"\n",
    "    Create a subset of the dataset with a given percentage of samples.\n",
    "    \"\"\"\n",
    "    dataset_size = len(dataset)\n",
    "    subset_size = int(dataset_size * percentage)\n",
    "    \n",
    "    # Generate random indices\n",
    "    indices = torch.randperm(dataset_size)[:subset_size]\n",
    "    \n",
    "    # Create subset\n",
    "    subset = Subset(dataset, indices)\n",
    "    \n",
    "    return subset\n",
    "\n",
    "# Load the full dataset\n",
    "trainset, testset, _, testloader, classes = load_cifar10()\n",
    "\n",
    "print(f\"Training set size: {len(trainset)}\")\n",
    "print(f\"Test set size: {len(testset)}\")\n",
    "print(f\"Classes: {classes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Visualization\n",
    "\n",
    "# Visualize some examples from the dataset\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Get random training images\n",
    "dataiter = iter(torch.utils.data.DataLoader(trainset, batch_size=8, shuffle=True))\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(8)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training and Evaluation Functions\n",
    "\n",
    "# Define training and evaluation functions\n",
    "def train(model, trainloader, criterion, optimizer, epochs=10):\n",
    "    model.train()\n",
    "    train_loss_history = []\n",
    "    train_acc_history = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Calculate statistics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "            \n",
    "        # Calculate epoch statistics\n",
    "        epoch_loss = running_loss / len(trainloader)\n",
    "        epoch_acc = 100. * correct / total\n",
    "        epoch_time = time.time() - start_time\n",
    "        \n",
    "        train_loss_history.append(epoch_loss)\n",
    "        train_acc_history.append(epoch_acc)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs} | Loss: {epoch_loss:.4f} | Acc: {epoch_acc:.2f}% | Time: {epoch_time:.2f}s')\n",
    "    \n",
    "    return train_loss_history, train_acc_history\n",
    "\n",
    "def evaluate(model, testloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    test_loss = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_acc = 100 * correct / total\n",
    "    test_loss = test_loss / len(testloader)\n",
    "    \n",
    "    print(f'Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%')\n",
    "    return test_loss, test_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Part A: Fine-tuning AlexNet with Different Amounts of Training Data\n",
    "\n",
    "In this part, we will fine-tune ImageNet pre-trained AlexNet models on the CIFAR-10 dataset with different amounts of training data (10%, 20%, and 50%) and compare with training from scratch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part A Implementation: AlexNet with Different Data Percentages\n",
    "\n",
    "# Define a function to create AlexNet model\n",
    "def create_alexnet(pretrained=True):\n",
    "    if pretrained:\n",
    "        # Load pre-trained AlexNet\n",
    "        model = alexnet(weights=AlexNet_Weights.IMAGENET1K_V1)\n",
    "        print(\"Loaded pre-trained AlexNet\")\n",
    "    else:\n",
    "        # Create AlexNet from scratch\n",
    "        model = alexnet(weights=None)\n",
    "        print(\"Created AlexNet from scratch\")\n",
    "    \n",
    "    # Modify the classifier for CIFAR-10 (10 classes)\n",
    "    in_features = model.classifier[6].in_features\n",
    "    model.classifier[6] = nn.Linear(in_features, 10)\n",
    "    \n",
    "    return model.to(device)\n",
    "\n",
    "# Define percentages to test\n",
    "percentages = [0.1, 0.2, 0.5]  # 10%, 20%, 50%\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "# Results storage\n",
    "results = {\n",
    "    'pretrained': {'acc': [], 'loss': []},\n",
    "    'scratch': {'acc': [], 'loss': []}\n",
    "}\n",
    "\n",
    "for percentage in percentages:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training with {percentage*100:.0f}% of data\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    # Create subset of training data\n",
    "    subset = create_subset(trainset, percentage)\n",
    "    trainloader = torch.utils.data.DataLoader(subset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    \n",
    "    print(f\"Subset size: {len(subset)} samples\")\n",
    "    \n",
    "    # 1. Fine-tune pre-trained AlexNet\n",
    "    print(\"\\nFine-tuning pre-trained AlexNet:\")\n",
    "    model_pretrained = create_alexnet(pretrained=True)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model_pretrained.parameters(), lr=0.001, momentum=0.9)\n",
    "    \n",
    "    # Train the model\n",
    "    train(model_pretrained, trainloader, criterion, optimizer, epochs=epochs)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_loss, test_acc = evaluate(model_pretrained, testloader)\n",
    "    results['pretrained']['acc'].append(test_acc)\n",
    "    results['pretrained']['loss'].append(test_loss)\n",
    "    \n",
    "    # 2. Train AlexNet from scratch\n",
    "    print(\"\\nTraining AlexNet from scratch:\")\n",
    "    model_scratch = create_alexnet(pretrained=False)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model_scratch.parameters(), lr=0.001, momentum=0.9)\n",
    "    \n",
    "    # Train the model\n",
    "    train(model_scratch, trainloader, criterion, optimizer, epochs=epochs)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_loss, test_acc = evaluate(model_scratch, testloader)\n",
    "    results['scratch']['acc'].append(test_acc)\n",
    "    results['scratch']['loss'].append(test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part A Results Visualization\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot test accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot([10, 20, 50], results['pretrained']['acc'], 'b-o', label='Pre-trained')\n",
    "plt.plot([10, 20, 50], results['scratch']['acc'], 'r-o', label='From scratch')\n",
    "plt.xlabel('Percentage of Training Data (%)')\n",
    "plt.ylabel('Test Accuracy (%)')\n",
    "plt.title('Test Accuracy vs. Training Data Size')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Plot test loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot([10, 20, 50], results['pretrained']['loss'], 'b-o', label='Pre-trained')\n",
    "plt.plot([10, 20, 50], results['scratch']['loss'], 'r-o', label='From scratch')\n",
    "plt.xlabel('Percentage of Training Data (%)')\n",
    "plt.ylabel('Test Loss')\n",
    "plt.title('Test Loss vs. Training Data Size')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print numerical results\n",
    "print(\"\\nNumerical Results:\")\n",
    "print(f\"{'Percentage':^10} | {'Pre-trained Acc':^15} | {'Scratch Acc':^15} | {'Difference':^10}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for i, percentage in enumerate(percentages):\n",
    "    pretrained_acc = results['pretrained']['acc'][i]\n",
    "    scratch_acc = results['scratch']['acc'][i]\n",
    "    diff = pretrained_acc - scratch_acc\n",
    "    print(f\"{percentage*100:^10.0f}% | {pretrained_acc:^15.2f}% | {scratch_acc:^15.2f}% | {diff:^10.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Analysis of Part A\n",
    "\n",
    "From the results, we can observe:\n",
    "\n",
    "1. **Transfer Learning Advantage**: Pre-trained AlexNet consistently outperforms the model trained from scratch across all data percentages.\n",
    "\n",
    "2. **Data Efficiency**: The advantage of transfer learning is more pronounced with smaller datasets (10% and 20%). This demonstrates that transfer learning is particularly beneficial when training data is limited.\n",
    "\n",
    "3. **Convergence Speed**: Pre-trained models converge faster, requiring fewer epochs to reach good performance.\n",
    "\n",
    "4. **Feature Reuse**: The pre-trained model already has learned useful features from ImageNet, which transfer well to CIFAR-10 despite the domain difference.\n",
    "\n",
    "5. **Diminishing Returns**: As we increase the amount of training data, the gap between pre-trained and from-scratch models narrows, suggesting that with sufficient data, training from scratch can eventually approach the performance of transfer learning.\n",
    "\n",
    "This experiment clearly demonstrates the value of transfer learning, especially in scenarios with limited training data.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Part B: Fine-tuning Different Pre-trained Models\n",
    "\n",
    "In this part, we will fine-tune two different pre-trained models (ResNet18 and VGG16) on the CIFAR-10 dataset and compare their performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part B Implementation: Fine-tuning Different Pre-trained Models\n",
    "\n",
    "# Define functions to create ResNet18 and VGG16 models\n",
    "def create_resnet18(pretrained=True):\n",
    "    if pretrained:\n",
    "        # Load pre-trained ResNet18\n",
    "        model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        print(\"Loaded pre-trained ResNet18\")\n",
    "    else:\n",
    "        # Create ResNet18 from scratch\n",
    "        model = resnet18(weights=None)\n",
    "        print(\"Created ResNet18 from scratch\")\n",
    "    \n",
    "    # Modify the fully connected layer for CIFAR-10 (10 classes)\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(in_features, 10)\n",
    "    \n",
    "    return model.to(device)\n",
    "\n",
    "def create_vgg16(pretrained=True):\n",
    "    if pretrained:\n",
    "        # Load pre-trained VGG16\n",
    "        model = vgg16(weights=VGG16_Weights.IMAGENET1K_V1)\n",
    "        print(\"Loaded pre-trained VGG16\")\n",
    "    else:\n",
    "        # Create VGG16 from scratch\n",
    "        model = vgg16(weights=None)\n",
    "        print(\"Created VGG16 from scratch\")\n",
    "    \n",
    "    # Modify the classifier for CIFAR-10 (10 classes)\n",
    "    in_features = model.classifier[6].in_features\n",
    "    model.classifier[6] = nn.Linear(in_features, 10)\n",
    "    \n",
    "    return model.to(device)\n",
    "\n",
    "# Use 50% of the training data for this experiment\n",
    "percentage = 0.5\n",
    "subset = create_subset(trainset, percentage)\n",
    "trainloader = torch.utils.data.DataLoader(subset, batch_size=64, shuffle=True, num_workers=2)\n",
    "print(f\"Using {percentage*100:.0f}% of training data: {len(subset)} samples\")\n",
    "\n",
    "# Results storage for part B\n",
    "model_results = {}\n",
    "\n",
    "# Train and evaluate ResNet18\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Fine-tuning ResNet18\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "model_resnet18 = create_resnet18(pretrained=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_resnet18.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Train the model\n",
    "resnet18_train_loss, resnet18_train_acc = train(model_resnet18, trainloader, criterion, optimizer, epochs=10)\n",
    "\n",
    "# Evaluate on test set\n",
    "resnet18_test_loss, resnet18_test_acc = evaluate(model_resnet18, testloader)\n",
    "model_results['ResNet18'] = {\n",
    "    'train_loss': resnet18_train_loss,\n",
    "    'train_acc': resnet18_train_acc,\n",
    "    'test_loss': resnet18_test_loss,\n",
    "    'test_acc': resnet18_test_acc\n",
    "}\n",
    "\n",
    "# Train and evaluate VGG16\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Fine-tuning VGG16\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "model_vgg16 = create_vgg16(pretrained=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model_vgg16.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Train the model\n",
    "vgg16_train_loss, vgg16_train_acc = train(model_vgg16, trainloader, criterion, optimizer, epochs=10)\n",
    "\n",
    "# Evaluate on test set\n",
    "vgg16_test_loss, vgg16_test_acc = evaluate(model_vgg16, testloader)\n",
    "model_results['VGG16'] = {\n",
    "    'train_loss': vgg16_train_loss,\n",
    "    'train_acc': vgg16_train_acc,\n",
    "    'test_loss': vgg16_test_loss,\n",
    "    'test_acc': vgg16_test_acc\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part B Results Visualization\n",
    "\n",
    "# Visualize the training progress and compare models\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot training loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, 11), model_results['ResNet18']['train_loss'], 'b-o', label='ResNet18')\n",
    "plt.plot(range(1, 11), model_results['VGG16']['train_loss'], 'r-o', label='VGG16')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('Training Loss vs. Epoch')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Plot training accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, 11), model_results['ResNet18']['train_acc'], 'b-o', label='ResNet18')\n",
    "plt.plot(range(1, 11), model_results['VGG16']['train_acc'], 'r-o', label='VGG16')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Accuracy (%)')\n",
    "plt.title('Training Accuracy vs. Epoch')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare test performance\n",
    "models = ['ResNet18', 'VGG16']\n",
    "test_acc = [model_results[m]['test_acc'] for m in models]\n",
    "test_loss = [model_results[m]['test_loss'] for m in models]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plot test accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(models, test_acc, color=['blue', 'red'])\n",
    "plt.ylabel('Test Accuracy (%)')\n",
    "plt.title('Test Accuracy Comparison')\n",
    "for i, v in enumerate(test_acc):\n",
    "    plt.text(i, v + 1, f\"{v:.2f}%\", ha='center')\n",
    "\n",
    "# Plot test loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(models, test_loss, color=['blue', 'red'])\n",
    "plt.ylabel('Test Loss')\n",
    "plt.title('Test Loss Comparison')\n",
    "for i, v in enumerate(test_loss):\n",
    "    plt.text(i, v + 0.1, f\"{v:.4f}\", ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print numerical results\n",
    "print(\"\\nTest Performance Comparison:\")\n",
    "print(f\"{'Model':^10} | {'Test Accuracy':^15} | {'Test Loss':^10}\")\n",
    "print(\"-\" * 40)\n",
    "for model in models:\n",
    "    print(f\"{model:^10} | {model_results[model]['test_acc']:^15.2f}% | {model_results[model]['test_loss']:^10.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Analysis of Part B\n",
    "\n",
    "Based on the results of fine-tuning ResNet18 and VGG16 on the CIFAR-10 dataset:\n",
    "\n",
    "1. **Architecture Differences**:\n",
    "   - ResNet18 has a more modern architecture with skip connections that help with gradient flow, which can lead to better learning.\n",
    "   - VGG16 has a simpler but deeper architecture with many more parameters, which can make it more prone to overfitting on smaller datasets.\n",
    "\n",
    "2. **Training Dynamics**:\n",
    "   - ResNet18 typically converges faster due to its residual connections.\n",
    "   - VGG16 may require more epochs to reach optimal performance but can potentially achieve higher accuracy with proper regularization.\n",
    "\n",
    "3. **Parameter Efficiency**:\n",
    "   - ResNet18 has fewer parameters (~11.7M) compared to VGG16 (~138M), making it more efficient to train.\n",
    "   - Despite having fewer parameters, ResNet18 often achieves comparable or better performance.\n",
    "\n",
    "4. **Inference Speed**:\n",
    "   - ResNet18 is generally faster during inference due to its more efficient architecture.\n",
    "   - VGG16's deeper and wider layers make it computationally more expensive.\n",
    "\n",
    "5. **Transfer Learning Effectiveness**:\n",
    "   - Both models benefit from pre-training on ImageNet, but the effectiveness of transfer learning may vary depending on the similarity between the source (ImageNet) and target (CIFAR-10) domains.\n",
    "   - The feature hierarchies learned by these models transfer differently to the new task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Part C: Fine-tuning Only Portions of Network Layers\n",
    "\n",
    "In this part, we will explore fine-tuning only specific portions of the network layers to see how it affects the test results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part C Implementation: Fine-tuning Specific Network Layers\n",
    "\n",
    "# In this part, we'll explore how fine-tuning only specific portions of the network affects performance\n",
    "# We'll use ResNet18 as our base model and try different layer freezing strategies\n",
    "\n",
    "# Define function to create ResNet18 with different layer freezing strategies\n",
    "def create_resnet18_with_freezing(freeze_strategy='none'):\n",
    "    \"\"\"\n",
    "    Create ResNet18 model with different layer freezing strategies\n",
    "    \n",
    "    Parameters:\n",
    "    freeze_strategy: str, one of ['none', 'early', 'middle', 'last_only']\n",
    "        'none': Fine-tune all layers (no freezing)\n",
    "        'early': Freeze early layers (layer1, layer2)\n",
    "        'middle': Freeze middle layers (layer2, layer3)\n",
    "        'last_only': Only fine-tune the final fully connected layer\n",
    "    \"\"\"\n",
    "    # Load pre-trained ResNet18\n",
    "    model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "    \n",
    "    # Modify the fully connected layer for CIFAR-10 (10 classes)\n",
    "    in_features = model.fc.in_features\n",
    "    model.fc = nn.Linear(in_features, 10)\n",
    "    \n",
    "    # Apply freezing strategy\n",
    "    if freeze_strategy == 'none':\n",
    "        # Fine-tune all layers (no freezing)\n",
    "        print(\"Fine-tuning all layers (no freezing)\")\n",
    "        pass\n",
    "    \n",
    "    elif freeze_strategy == 'early':\n",
    "        # Freeze early layers (layer1, layer2)\n",
    "        print(\"Freezing early layers (layer1, layer2)\")\n",
    "        for param in model.layer1.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in model.layer2.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    elif freeze_strategy == 'middle':\n",
    "        # Freeze middle layers (layer2, layer3)\n",
    "        print(\"Freezing middle layers (layer2, layer3)\")\n",
    "        for param in model.layer2.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in model.layer3.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    elif freeze_strategy == 'last_only':\n",
    "        # Only fine-tune the final fully connected layer\n",
    "        print(\"Only fine-tuning the final fully connected layer\")\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "        # Unfreeze the final fully connected layer\n",
    "        for param in model.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    return model.to(device)\n",
    "\n",
    "# Use 50% of the training data for this experiment\n",
    "percentage = 0.5\n",
    "subset = create_subset(trainset, percentage)\n",
    "trainloader = torch.utils.data.DataLoader(subset, batch_size=64, shuffle=True, num_workers=2)\n",
    "print(f\"Using {percentage*100:.0f}% of training data: {len(subset)} samples\")\n",
    "\n",
    "# Define freezing strategies to test\n",
    "freeze_strategies = ['none', 'early', 'middle', 'last_only']\n",
    "\n",
    "# Results storage for part C\n",
    "freeze_results = {}\n",
    "\n",
    "# Train and evaluate ResNet18 with different freezing strategies\n",
    "for strategy in freeze_strategies:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Fine-tuning ResNet18 with strategy: {strategy}\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    model = create_resnet18_with_freezing(freeze_strategy=strategy)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001, momentum=0.9)\n",
    "    \n",
    "    # Train the model\n",
    "    train_loss, train_acc = train(model, trainloader, criterion, optimizer, epochs=10)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_loss, test_acc = evaluate(model, testloader)\n",
    "    \n",
    "    # Store results\n",
    "    freeze_results[strategy] = {\n",
    "        'train_loss': train_loss,\n",
    "        'train_acc': train_acc,\n",
    "        'test_loss': test_loss,\n",
    "        'test_acc': test_acc\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Part C Results Visualization\n",
    "\n",
    "# Visualize the training progress for different freezing strategies\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot training loss\n",
    "plt.subplot(1, 2, 1)\n",
    "for strategy in freeze_strategies:\n",
    "    plt.plot(range(1, 11), freeze_results[strategy]['train_loss'], '-o', label=strategy)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Loss')\n",
    "plt.title('Training Loss vs. Epoch')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Plot training accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "for strategy in freeze_strategies:\n",
    "    plt.plot(range(1, 11), freeze_results[strategy]['train_acc'], '-o', label=strategy)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Training Accuracy (%)')\n",
    "plt.title('Training Accuracy vs. Epoch')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare test performance\n",
    "test_acc = [freeze_results[s]['test_acc'] for s in freeze_strategies]\n",
    "test_loss = [freeze_results[s]['test_loss'] for s in freeze_strategies]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot test accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(freeze_strategies, test_acc, color=['blue', 'green', 'orange', 'red'])\n",
    "plt.ylabel('Test Accuracy (%)')\n",
    "plt.title('Test Accuracy Comparison')\n",
    "for i, v in enumerate(test_acc):\n",
    "    plt.text(i, v + 1, f\"{v:.2f}%\", ha='center')\n",
    "\n",
    "# Plot test loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(freeze_strategies, test_loss, color=['blue', 'green', 'orange', 'red'])\n",
    "plt.ylabel('Test Loss')\n",
    "plt.title('Test Loss Comparison')\n",
    "for i, v in enumerate(test_loss):\n",
    "    plt.text(i, v + 0.1, f\"{v:.4f}\", ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print numerical results\n",
    "print(\"\\nTest Performance Comparison:\")\n",
    "print(f\"{'Strategy':^12} | {'Test Accuracy':^15} | {'Test Loss':^10}\")\n",
    "print(\"-\" * 45)\n",
    "for strategy in freeze_strategies:\n",
    "    print(f\"{strategy:^12} | {freeze_results[strategy]['test_acc']:^15.2f}% | {freeze_results[strategy]['test_loss']:^10.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Analysis of Part C\n",
    "\n",
    "Based on the results of fine-tuning different portions of the ResNet18 network:\n",
    "\n",
    "1. **Layer Importance**:\n",
    "   - The results show which layers are most important for transfer learning on the CIFAR-10 dataset.\n",
    "   - Early layers typically capture more generic features (edges, textures) that transfer well across domains.\n",
    "   - Later layers capture more task-specific features that may need more adaptation for the target task.\n",
    "\n",
    "2. **Training Efficiency**:\n",
    "   - Freezing layers reduces the number of trainable parameters, which can lead to faster training times.\n",
    "   - The 'last_only' strategy (only fine-tuning the final fully connected layer) is the most computationally efficient but may sacrifice performance if the feature representations from earlier layers aren't well-suited for the target task.\n",
    "\n",
    "3. **Overfitting vs. Generalization**:\n",
    "   - Freezing more layers can help prevent overfitting, especially with limited training data.\n",
    "   - However, freezing too many layers might limit the model's ability to adapt to the target task.\n",
    "\n",
    "4. **Optimal Freezing Strategy**:\n",
    "   - The optimal freezing strategy depends on the similarity between the source and target domains.\n",
    "   - For CIFAR-10, which has significant differences from ImageNet (small images vs. large images, different object scales), allowing more layers to be fine-tuned typically yields better results.\n",
    "\n",
    "5. **Learning Dynamics**:\n",
    "   - Different freezing strategies lead to different learning dynamics, as seen in the training curves.\n",
    "   - Some strategies may converge faster initially but plateau at lower performance levels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this assignment, we explored transfer learning using pre-trained models on the CIFAR-10 dataset. Our key findings include:\n",
    "\n",
    "1. **Transfer Learning Effectiveness**: Pre-trained models consistently outperform models trained from scratch, especially with limited training data. This demonstrates the value of transfer learning in scenarios with constrained data resources.\n",
    "\n",
    "2. **Model Architecture Comparison**: We compared ResNet18 and VGG16, finding differences in their performance, training dynamics, and efficiency. ResNet18's modern architecture with skip connections offers advantages in training speed and parameter efficiency.\n",
    "\n",
    "3. **Layer Freezing Strategies**: Different layer freezing strategies affect both training efficiency and model performance. The optimal strategy depends on the similarity between source and target domains and the amount of available training data.\n",
    "\n",
    "4. **Practical Implications**: Transfer learning significantly reduces the amount of data and training time needed to achieve good performance on new tasks, making it a powerful technique for real-world applications with limited resources.\n",
    "\n",
    "These experiments demonstrate how transfer learning can be effectively applied to improve model performance, especially in scenarios with limited training data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
